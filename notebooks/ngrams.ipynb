{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efab651-b4ab-472e-bc7c-83a57db2b19a",
   "metadata": {},
   "source": [
    "## 4.2 N-grams Features ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39250be-dd85-4030-8fb9-0fa9ee22eca2",
   "metadata": {},
   "source": [
    "#### Import Packages ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7cab2f-d6e8-4945-ad4b-e2cddb18e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcbf4a-01d4-478e-a1d0-a206d8a375d0",
   "metadata": {},
   "source": [
    "#### Load and Split Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28285cef-d5e4-4ce1-897b-c9617c5bf802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset missing values:\n",
      "is_sarcastic    0\n",
      "headline        0\n",
      "article_link    0\n",
      "dtype: int64\n",
      "Sarcastic count: 13634\n",
      "Non-sarcastic count: 14985\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('../data/Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
    "X = df['headline']\n",
    "y = df['is_sarcastic']\n",
    "\n",
    "# Split data: 80% training, 10% validation and 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=50, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=50, stratify=y_temp)\n",
    "#random_seeds = [50, 98, 54, 6, 34] # generated using random seed=0 \n",
    "\n",
    "print(\"Dataset missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"Sarcastic count:\", df[df['is_sarcastic'] == 1].shape[0])\n",
    "print(\"Non-sarcastic count:\", df[df['is_sarcastic'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784ca494-bb12-463c-b20f-89f67ed91d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    show_metrics(model, X_train, y_train, 'training set')\n",
    "    show_metrics(model, X_val, y_val, \"validation set\")\n",
    "    \n",
    "def show_metrics(model, X, y, label_str):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(f\"{label_str} metrics:\")\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    print(\"Accuracy = {:.4f}\".format(acc))\n",
    "\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "    print(\"Macro F1 = {:.4f}\".format(f1))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100c1f1-b4c6-473d-b5d2-fea873a996f5",
   "metadata": {},
   "source": [
    "#### Choosing Models (NB, LR, NN) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf05fd1-c274-4775-b7ae-d8099dbaeeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9300\n",
      "Macro F1 = 0.9298\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8477\n",
      "Macro F1 = 0.8470\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "NB_model = make_pipeline(CountVectorizer(ngram_range=(1,1)), MultinomialNB())\n",
    "train_and_evaluate(NB_model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6bee07-6a57-42ff-925d-9797d28b6fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9610\n",
      "Macro F1 = 0.9609\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8435\n",
      "Macro F1 = 0.8429\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "LR_model = make_pipeline(CountVectorizer(ngram_range=(1,1)), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(LR_model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345e193b-f4ae-4a32-bdc0-618bf10a685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9281\n",
      "Macro F1 = 0.9279\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8487\n",
      "Macro F1 = 0.8483\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "NN_model = make_pipeline(CountVectorizer(ngram_range=(1,1)), MLPClassifier(hidden_layer_sizes=(15,), max_iter=70, early_stopping=True))\n",
    "train_and_evaluate(NN_model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d30a8-10cc-49d1-800f-c0ea987f4216",
   "metadata": {},
   "source": [
    "#### Baseline Model ####\n",
    "Choose LR\n",
    "- is linear, can directly interpret the feature weights\n",
    "- NB, NN, LR have similar macro F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470a1af8-4b20-4e25-b1eb-28840d4c2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9610\n",
      "Macro F1 = 0.9609\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8435\n",
      "Macro F1 = 0.8429\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "LR_model = make_pipeline(CountVectorizer(ngram_range=(1,1)), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(LR_model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfd520-ff95-4a1b-b70b-9cf421a590d6",
   "metadata": {},
   "source": [
    "#### Experimental Model ####\n",
    "Choose ngram_range=(1, 2)\n",
    "macro F1 = 0.8544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d45a648-5e4c-4707-abc0-aea1ae166095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9990\n",
      "Macro F1 = 0.9989\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8550\n",
      "Macro F1 = 0.8544\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model1 = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(model1, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94ad9ec-b959-4c85-a70e-9e45d2d54fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9995\n",
      "Macro F1 = 0.9995\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.7764\n",
      "Macro F1 = 0.7731\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model2 = make_pipeline(CountVectorizer(ngram_range=(2,2)), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(model2, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03c1984-3727-4a89-86d2-ea98912ab097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 1.0000\n",
      "Macro F1 = 1.0000\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.7722\n",
      "Macro F1 = 0.7681\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model3 = make_pipeline(CountVectorizer(ngram_range=(2,3)), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(model3, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f9a242-53ee-4959-8213-dff08b6d1c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9985\n",
      "Macro F1 = 0.9985\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.6279\n",
      "Macro F1 = 0.5719\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model4 = make_pipeline(CountVectorizer(ngram_range=(3,3)), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(model4, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a9147-acd3-4ae7-a6c7-0eb7940b0e2d",
   "metadata": {},
   "source": [
    "#### Preprocessing + Hyperparameter Finetuning #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "342abb7f-8e2c-4b5c-aa4f-e1bf908b6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9976\n",
      "Macro F1 = 0.9976\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8012\n",
      "Macro F1 = 0.7988\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2), stop_words='english'), LogisticRegression(max_iter=150))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c01d07bd-20ea-4b30-8830-a8be04659b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumch\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9135\n",
      "Macro F1 = 0.9133\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8393\n",
      "Macro F1 = 0.8387\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=200, penalty='l1', solver='saga'))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c962964-5151-4faf-a48e-41a0cc5fbed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumch\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9995\n",
      "Macro F1 = 0.9995\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8512\n",
      "Macro F1 = 0.8506\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=300, penalty='l1', solver='saga', C=10))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82a3a50-2860-4a61-881e-61b6ac7fd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9982\n",
      "Macro F1 = 0.9982\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8546\n",
      "Macro F1 = 0.8540\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150, C=0.8))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfde0fe-7adf-4100-9fb8-e370bf671a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9962\n",
      "Macro F1 = 0.9962\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8539\n",
      "Macro F1 = 0.8534\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150, C=0.6))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d07ac1-30d1-4eaa-aa80-04f1b91cc869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9943\n",
      "Macro F1 = 0.9943\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8550\n",
      "Macro F1 = 0.8545\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150, C=0.5))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c5c801-1030-4eb3-b54d-0f6ee49163b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9904\n",
      "Macro F1 = 0.9904\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8522\n",
      "Macro F1 = 0.8517\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150, C=0.4))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3165a26c-dc53-4153-ab35-53e80c47e364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9842\n",
      "Macro F1 = 0.9842\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8515\n",
      "Macro F1 = 0.8510\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150, C=0.3))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6aeda-aeac-4d10-9fcf-b2bd1864a3c9",
   "metadata": {},
   "source": [
    "#### Model Performance on Test Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92141da3-6cbd-4ccb-901f-83051f43ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set metrics:\n",
      "Accuracy = 0.9943\n",
      "Macro F1 = 0.9943\n",
      "--------------------------------------------------\n",
      "validation set metrics:\n",
      "Accuracy = 0.8550\n",
      "Macro F1 = 0.8545\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(max_iter=150, C=0.5))\n",
    "train_and_evaluate(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "014eb951-2097-4cae-9793-cd6049bbb882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing set metrics:\n",
      "Accuracy = 0.8498\n",
      "Macro F1 = 0.8495\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "show_metrics(model, X_test, y_test, 'testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76153a7-f114-4826-933d-13ddd5a94e47",
   "metadata": {},
   "source": [
    "#### Top 10 Features ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaa37fca-a866-4f14-9be3-72f0c74f15d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features indicative of sarcasm:\n",
      "nation: 2.9191\n",
      "area: 2.5409\n",
      "local: 1.9461\n",
      "onion: 1.7815\n",
      "fucking: 1.7719\n",
      "introduces: 1.7114\n",
      "announces: 1.7062\n",
      "report: 1.6971\n",
      "only: 1.6423\n",
      "clearly: 1.5863\n"
     ]
    }
   ],
   "source": [
    "vectorizer = model.named_steps['countvectorizer']\n",
    "classifier = model.named_steps['logisticregression']\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = classifier.coef_[0]\n",
    "\n",
    "top10_idx = np.argsort(coefficients)[-10:]\n",
    "top10_features = feature_names[top10_idx]\n",
    "top10_weights = coefficients[top10_idx]\n",
    "\n",
    "print(\"Top 10 features indicative of sarcasm:\")\n",
    "for feature, weight in zip(top10_features[::-1], top10_weights[::-1]):\n",
    "    print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8613680-0ac9-41ee-9cfa-0d93a4f624a5",
   "metadata": {},
   "source": [
    "## Research Question ##\n",
    "#### N-gram Corpus Analysis ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc050f13-e660-411b-9695-620a3643d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word  sarcastic_count  nonsarcastic_count  sarcastic_percentage\n",
      "0      nation              389                  23              0.944175\n",
      "1        area              490                  11              0.978044\n",
      "2       local              158                  14              0.918605\n",
      "3       onion               56                   1              0.982456\n",
      "4     fucking              102                   0              1.000000\n",
      "5  introduces              112                   9              0.925620\n",
      "6   announces              134                  19              0.875817\n",
      "7      report              516                  89              0.852893\n",
      "8        only              221                  56              0.797834\n",
      "9     clearly               56                   0              1.000000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "top_features = top10_features[::-1].tolist()\n",
    "\n",
    "sarcastic_headlines = df[df['is_sarcastic'] == 1]['headline']\n",
    "nonsarcastic_headlines = df[df['is_sarcastic'] == 0]['headline']\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "def count_words(texts, words):\n",
    "    counts = Counter()\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "    for line in texts:\n",
    "        ngram_tokens = analyzer(line)\n",
    "        for word in words:\n",
    "            counts[word] += ngram_tokens.count(word)\n",
    "    return counts\n",
    "\n",
    "sarcastic_counts = count_words(sarcastic_headlines, top_features)\n",
    "nonsarcastic_counts = count_words(nonsarcastic_headlines, top_features)\n",
    "\n",
    "# for comparison\n",
    "freq_df = pd.DataFrame({\n",
    "    'word': top_features,\n",
    "    'sarcastic_count': [sarcastic_counts[w] for w in top_features],\n",
    "    'nonsarcastic_count': [nonsarcastic_counts[w] for w in top_features]\n",
    "})\n",
    "freq_df['sarcastic_percentage'] = freq_df['sarcastic_count'] / (freq_df['sarcastic_count'] + freq_df['nonsarcastic_count'])\n",
    "\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf2616-6f28-4a8c-beda-03ae52c7b479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
